# -*- coding: utf-8 -*-
"""ML_workshop.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1FztmwoNZZbl_wiPx3Thu9f1PcPtYQc9N
"""

# Load CSV using Pandas
import pandas as pd
filename = '/content/diabetes.csv'
#names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class'] 
#data = pd.read_csv(filename, names=names)
df = pd.read_csv(filename)

#Dimensions of Your Data
print(df.shape)

#Peek at Your Data
#peek = df.head(20)
peek=df.tail(5)
peek

df.columns=['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class'] 
df

#Data Type For Each Attribute
types = df.dtypes
print(types)

df["mass"] = df["mass"].astype(str).astype(float)
df["pedi"] = df["pedi"].astype(str).astype(float)

types = df.dtypes
print(types)

#Descriptive Statistics
pd.set_option('display.width', 100)
#pd.set_option('precision', 3)
description = df.describe()
print(description)

df.shape

#Class Distribution (Classification Only)
class_counts = df.groupby('class').size()
print(class_counts)

#Correlations Between Attributes
correlations = df.corr(method='pearson')
print(correlations)

#Skew of Univariate Distributions
skew = df.skew()
print(skew)

#Data Visualidations#omit
#Histograms
from matplotlib import pyplot
df.hist()
pyplot.show

#density plot#omit
df.plot(kind='density', subplots=True, layout=(3,3), sharex=False) 
pyplot.show()

#Box and whisker plot#omit
df.plot(kind='box', subplots=True, layout=(3,3), sharex=False, sharey=False) 
pyplot.show()

#Multivariate plots#omit
import numpy as np
names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']
correlations=df.corr()
fig = pyplot.figure()
ax = fig.add_subplot(111)
cax = ax.matshow(correlations, vmin=-1, vmax=1)
fig.colorbar(cax)
ticks = np.arange(0,9,1)
ax.set_xticks(ticks)
ax.set_yticks(ticks)
ax.set_xticklabels(names)
ax.set_yticklabels(names)
pyplot.show()

# Rescale data (between 0 and 1)
from pandas import read_csv
from numpy import set_printoptions
from sklearn.preprocessing import MinMaxScaler
filename = '/content/diabetes.csv'

dataframe = read_csv(filename)
array = dataframe.values
# separate array into input and output components
X = array[:,0:8]
Y = array[:,8]
scaler = MinMaxScaler(feature_range=(0, 1))
rescaledX = scaler.fit_transform(X)
# summarize transformed data
set_printoptions(precision=3)
print(rescaledX[0:5,:])
X=rescaledX

from sklearn.feature_selection import SelectKBest
from sklearn.feature_selection import chi2
array = df.values
# separate array into input and output components
X = array[:,0:8]
Y = array[:,8]
test = SelectKBest(score_func=chi2, k=4)
fit = test.fit(X, Y)
# summarize scores
set_printoptions(precision=3)
print(fit.scores_)
features = fit.transform(X)
# summarize selected features
print(features[0:5,:])

# feature extraction
from sklearn.feature_selection import RFE
from sklearn.linear_model import LogisticRegression
model = LogisticRegression()
rfe = RFE(model,n_features_to_select=4, step=1)
fit = rfe.fit(X, Y)
#print("Num Features: %d") % fit.n_features_
#print("Selected Features: %s") % fit.support_
#print("Feature Ranking: %s") % fit.ranking_
feat=fit.n_features_
filter = fit.support_
ranking = fit.ranking_
print("Number of features:",feat)
print("Selected features: ", filter)
print("Feature Ranking: ", ranking)

from scipy.special import modfresnelp
import pandas as pd
from sklearn.ensemble import RandomForestClassifier       
forest = RandomForestClassifier()
forest.fit(X, Y)
importances = forest.feature_importances_
importances

import matplotlib.pyplot as plt
std = np.std([tree.feature_importances_ for tree in forest.estimators_],axis=0)
indices = np.argsort(importances)

# Plot the feature importances of the forest
plt.figure()
plt.title("Feature importances")
plt.barh(range(X.shape[1]), importances[indices],
       color="r", xerr=std[indices], align="center")
# If you want to define your own labels,
# change indices to a list of labels on the following line.
plt.yticks(range(X.shape[1]), indices)
plt.ylim([-1, X.shape[1]])
plt.show()

name_drop=['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age'] 
(pd.Series(forest.feature_importances_, index=name_drop)
   .nlargest(8)
   .plot(kind='barh'))

from pandas import read_csv
from numpy import set_printoptions
from sklearn.preprocessing import MinMaxScaler
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import KFold
filename = '/content/diabetes.csv'
dataframe = read_csv(filename)
array = dataframe.values
# separate array into input and output components
X = array[:,0:8]
Y = array[:,8]
kfold = KFold(n_splits=10, random_state=7,shuffle=True)
model = DecisionTreeClassifier(criterion="entropy",random_state=0)
scoring_accuracy='accuracy'
scoring = 'neg_log_loss'
scoring_roc = 'roc_auc'
res = cross_val_score(model, X, Y, cv=kfold, scoring=scoring_accuracy) 
results = cross_val_score(model, X, Y, cv=kfold, scoring=scoring) 
res_auc=cross_val_score(model, X, Y, cv=kfold, scoring=scoring_roc) 
print("Accuracy:",res.mean())
print("Logloss: ",results.mean()) 
print("AUC:", res_auc.mean())

from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix
from sklearn.metrics import classification_report
test_size = 0.33
seed = 7
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=test_size,random_state=seed)
model = DecisionTreeClassifier(criterion="entropy",random_state=0)
model.fit(X_train, Y_train)
predicted = model.predict(X_test)
matrix = confusion_matrix(Y_test, predicted)
print(matrix)

from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix
from sklearn.metrics import classification_report
test_size = 0.33
seed = 7
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=test_size,random_state=seed)
model = DecisionTreeClassifier(criterion="entropy",random_state=0)
model.fit(X_train, Y_train)
report = classification_report(Y_test, predicted)
print(report)

# Importing all necessary libraries
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_absolute_error
from sklearn.metrics import mean_squared_error

test_size = 0.33
seed = 7
X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=test_size,random_state=seed)
# Initializing the model and fitting the model with train data
model = LinearRegression()
model.fit(X_train,y_train)
# Generating predictions over test data
predictions = model.predict(X_test)
# Evaluating the model using MAE Evaluation Metric
print("MAE:",mean_absolute_error(y_test, predictions))
print("MSE:",mean_squared_error(y_test, predictions))

# Compare Algorithms
from pandas import read_csv
from matplotlib import pyplot
from sklearn.model_selection import KFold
from sklearn.model_selection import cross_val_score
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.tree import DecisionTreeClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.svm import SVC
# load dataset
filename = '/content/diabetes.csv'
names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class'] 
dataframe = read_csv(filename, names=names)
array = dataframe.values
X = array[:,0:8]
Y = array[:,8]
# prepare models


models = []

models.append(('KNN', KNeighborsClassifier()))
models.append(('NB', GaussianNB()))
models.append(('CART', DecisionTreeClassifier()))
models.append(('SVM', SVC()))
# evaluate each model in turn


results = []
names = []
scoring = 'accuracy'
for name, model in models:
  kfold = KFold(n_splits=10, random_state=7,shuffle=True)
  scoring_acc = 'accuracy'
  res = cross_val_score(model, X, Y, cv=kfold, scoring=scoring_accuracy) 
  results.append(res)
  names.append(name)
  print("Name:",name , "Accuracy:", res.mean() )
# boxplot algorithm comparison
fig = pyplot.figure() 
fig.suptitle('Algorithm Comparison') 
ax = fig.add_subplot(111) 
pyplot.boxplot(results) 
ax.set_xticklabels(names) 
pyplot.show()











# Cross Validation Classification Accuracy  #omit chekc
from pandas import read_csv
from sklearn.model_selection import KFold
from sklearn.model_selection import cross_val_score
from sklearn.linear_model import LogisticRegression
filename = '/content/diabetes.csv'
names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class'] 
dataframe = read_csv(filename, names=names)
array = dataframe.values
X = array[:,0:8]
Y = array[:,8]

from sklearn.model_selection import cross_validate
def cross_validation(model, _X, _y, _cv=5):
      '''Function to perform 5 Folds Cross-Validation
       Parameters
       ----------
      model: Python Class, default=None
              This is the machine learning algorithm to be used for training.
      _X: array
           This is the matrix of features.
      _y: array
           This is the target variable.
      _cv: int, default=5
          Determines the number of folds for cross-validation.
       Returns
       -------
       The function returns a dictionary containing the metrics 'accuracy', 'precision',
       'recall', 'f1' for both training set and validation set.
      '''
      _scoring = ['accuracy', 'precision', 'recall', 'f1']
      results = cross_validate(estimator=model,
                               X=_X,
                               y=_y,
                               cv=_cv,
                               scoring=_scoring,
                               return_train_score=True)
      
      return {"Training Accuracy scores": results['train_accuracy'],
              "Mean Training Accuracy": results['train_accuracy'].mean()*100,
              "Training Precision scores": results['train_precision'],
              "Mean Training Precision": results['train_precision'].mean(),
              "Training Recall scores": results['train_recall'],
              "Mean Training Recall": results['train_recall'].mean(),
              "Training F1 scores": results['train_f1'],
              "Mean Training F1 Score": results['train_f1'].mean(),
              "Validation Accuracy scores": results['test_accuracy'],
              "Mean Validation Accuracy": results['test_accuracy'].mean()*100,
              "Validation Precision scores": results['test_precision'],
              "Mean Validation Precision": results['test_precision'].mean(),
              "Validation Recall scores": results['test_recall'],
              "Mean Validation Recall": results['test_recall'].mean(),
              "Validation F1 scores": results['test_f1'],
              "Mean Validation F1 Score": results['test_f1'].mean()
              }

def plot_result(x_label, y_label, plot_title, train_data, val_data):
        '''Function to plot a grouped bar chart showing the training and validation
          results of the ML model in each fold after applying K-fold cross-validation.
         Parameters
         ----------
         x_label: str, 
            Name of the algorithm used for training e.g 'Decision Tree'
          
         y_label: str, 
            Name of metric being visualized e.g 'Accuracy'
         plot_title: str, 
            This is the title of the plot e.g 'Accuracy Plot'
         
         train_result: list, array
            This is the list containing either training precision, accuracy, or f1 score.
        
         val_result: list, array
            This is the list containing either validation precision, accuracy, or f1 score.
         Returns
         -------
         The function returns a Grouped Barchart showing the training and validation result
         in each fold.
        '''
        
        # Set size of plot
        plt.figure(figsize=(12,6))
        labels = ["1st Fold", "2nd Fold", "3rd Fold", "4th Fold", "5th Fold"]
        X_axis = np.arange(len(labels))
        ax = plt.gca()
        plt.ylim(0.40000, 1)
        plt.bar(X_axis-0.2, train_data, 0.4, color='blue', label='Training')
        plt.bar(X_axis+0.2, val_data, 0.4, color='red', label='Validation')
        plt.title(plot_title, fontsize=30)
        plt.xticks(X_axis, labels)
        plt.xlabel(x_label, fontsize=14)
        plt.ylabel(y_label, fontsize=14)
        plt.legend()
        plt.grid(True)
        plt.show()

